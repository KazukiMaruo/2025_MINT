{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Libraries\n",
    "import sys, os\n",
    "import mne # Python package for processing and analyzing electrophysiological data\n",
    "import numpy as np\n",
    "from glob import glob # look for all the pathnames matching a specified pattern according to the rules\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA # ICA (Independent Component Analysis) algorithm, which is for artifact removal\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import permutation_test\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ~~~~~~~~~~~~~~ Libraries ~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual data is processed\n",
      "Machine learning parameters:\n",
      "  Window size: 1 samples (2 ms)\n",
      "  Classifier: LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~ Parameters\n",
    "group = 'adult'\n",
    "modality = 'visual' # 'visual' or 'audio'\n",
    "\n",
    "# ML parameters\n",
    "window_size = 1 # 1 sample = 2ms, 5 samples = 10 ms\n",
    "n_splits = 10\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Print out each parameter\n",
    "print(f\"{modality} data is processed\")\n",
    "print(\"Machine learning parameters:\")\n",
    "print(f\"  Window size: {window_size} samples ({window_size * 2} ms)\")\n",
    "print(f\"  Classifier: {clf.__class__.__name__}\")\n",
    "# ~~~~~~~~~~~~~~ Parameters ~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Set the working directory\n",
    "path = f\"/u/kazma/MINT/data/{group}/interim/{modality}\"\n",
    "sub_folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "sub_folders_sorted = sorted(sub_folders, key=lambda x: int(re.search(r'\\d+', x).group())) # Sort the folders based on the numeric part after \"sub-\"\n",
    "# ~~~~~~~~~~~~~~ Set the working directory ~~~~~~~~~~~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /u/kazma/MINT/data/adult/interim/visual/sub-01/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "1260 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "### SUB_LOOP\n",
    "subject = sub_folders_sorted[0]\n",
    "\n",
    "# each subject's file name\n",
    "sub_filename = os.path.join(path, subject, 'epochs-epo.fif') \n",
    "# Load epochs\n",
    "epochs = mne.read_epochs(sub_filename, preload=True)\n",
    "\n",
    "# Get the info about the data\n",
    "conditions = list(epochs.event_id.keys()) # list of conditions\n",
    "n_conditions = len(conditions) # number of conditions\n",
    "n_trials = len(epochs) # number of trials\n",
    "n_samples = epochs.get_data().shape[2]\n",
    "min_time = epochs.times[0]*1000   # First time point in milli seconds\n",
    "max_time = epochs.times[-1]*1000    # Last time point in milli seconds\n",
    "\n",
    "# list for avg accuracy for each time window\n",
    "all_decoding_accuracy = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Time point loop\n",
    "for timepoint in range(n_samples): # Loop over the each time point\n",
    "\n",
    "    # Initialize the decoding dictionary with each condition containing a list of flattened sample windows\n",
    "    decoding_dict = {}\n",
    "\n",
    "    for condition in conditions:\n",
    "        x = epochs[condition] \n",
    "        x = x.get_data() # the shape: (n_trials, n_channels, n_times)\n",
    "        x = x[:,:,timepoint] # extract one time point = 31 (channels) * 210 (trials)\n",
    "        decoding_dict[condition] = x\n",
    "\n",
    "    # Dictionary to store decoding results for each pair\n",
    "    pairwise_decoding_accuracies = {}\n",
    "\n",
    "    for i in range(n_conditions):\n",
    "        for j in range(i + 1, n_conditions):\n",
    "\n",
    "            cond1 = conditions[i]\n",
    "            cond2 = conditions[j]\n",
    "                \n",
    "            # Prepare data\n",
    "            data_cond1 = decoding_dict[cond1]\n",
    "            data_cond2 = decoding_dict[cond2]\n",
    "            data = np.vstack((data_cond1, data_cond2))\n",
    "            labels = np.hstack((np.zeros(len(data_cond1)), np.ones(len(data_cond2))))\n",
    "\n",
    "            # Set cross-validation\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) # StratifiedKFold ensures that each fold has a proportional representation of both classes, so each fold maintains a 50:50 balance of numerosity 1 and numerosity 2 trials.\n",
    "\n",
    "            # Time-resolved decoding storage\n",
    "            decoding_accuracies = []\n",
    "\n",
    "            # Cross-validation\n",
    "            for train_idx, test_idx in cv.split(data, labels):\n",
    "                X_train, X_test = data[train_idx], data[test_idx] # the EEG data for training and testing.\n",
    "                y_train, y_test = labels[train_idx], labels[test_idx] # corresponding labels for the training and testing data.\n",
    "\n",
    "                # standardization\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train) # get mean and standard deviation from this\n",
    "                X_test = scaler.transform(X_test) # get mean and standard deviation from this\n",
    "\n",
    "                # Train logistic regression\n",
    "                clf = LogisticRegression(max_iter=2000)\n",
    "                clf.fit(X_train, y_train) # trains the classifier on the training data (X_train) with labels (y_train).\n",
    "                accuracy = clf.score(X_test, y_test)\n",
    "                decoding_accuracies.append(accuracy) # calculates the classification accuracy by comparing the true labels (y_test) with the predicted labels (y_pred).\n",
    "\n",
    "            # average the accuracy within a time point\n",
    "            avg_accuracy = np.mean(decoding_accuracies)\n",
    "\n",
    "            # Store the average accuracy for the condition pair\n",
    "            pairwise_decoding_accuracies[(cond1, cond2)] = avg_accuracy\n",
    "\n",
    "            print(f\"Average accuracy for {cond1} vs {cond2} in time points {timepoint}: {avg_accuracy:.2f}\")\n",
    "\n",
    "    # Average accuracy for this time point across CV folds\n",
    "    all_decoding_accuracy.append(pairwise_decoding_accuracies)\n",
    "    print(f\"Time point {timepoint} is done\")\n",
    "# ~~~~~~~~~~~~~~ WINDOW_LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-01: saved in 'accuracy_pairwise.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy\n",
    "save_folder = f\"/u/kazma/MINT/data/adult/processed/{modality}/{subject}\"\n",
    "save_path = os.path.join(save_folder, \"accuracy_pairwise.pkl\") #  a pickle file\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(all_decoding_accuracy, f)\n",
    "print(f\"{subject}: saved in 'accuracy_pairwise.pkl'\")\n",
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy ~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~ Generate Accuracy matrix\n",
    "\n",
    "all_accuracy_matrices = [] # List to store the resulting accuracy matrices\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~  CONDITION_LOOP\n",
    "for idx, x in enumerate(all_decoding_accuracy):\n",
    "    # Extract all unique conditions\n",
    "    conditions = sorted(set(key[0] for key in x.keys()).union(set(key[1] for key in x.keys())))\n",
    "    n_conditions = len(conditions)\n",
    "\n",
    "    # Create a dictionary to map conditions to matrix indices\n",
    "    condition_idx = {condition: idx for idx, condition in enumerate(conditions)}\n",
    "\n",
    "    # Initialize a square matrix with NaN values (for easier filling)\n",
    "    accuracy_matrix = np.full((n_conditions, n_conditions), np.nan)\n",
    "\n",
    "    # Fill the matrix with accuracies\n",
    "    for (cond1, cond2), accuracy in x.items(): # e.g., 'cond1' = 'numerosity 1' and 'cond2' = 'numerosity 2'\n",
    "        i, j = condition_idx[cond1], condition_idx[cond2]\n",
    "        accuracy_matrix[i, j] = accuracy\n",
    "        accuracy_matrix[j, i] = accuracy  # Ensuring symmetry\n",
    "\n",
    "    # Convert to DataFrame for readability\n",
    "    accuracy_df = pd.DataFrame(accuracy_matrix, index=conditions, columns=conditions)\n",
    "    \n",
    "    # Store the DataFrame in the list\n",
    "    all_accuracy_matrices.append(accuracy_df)\n",
    "\n",
    "    # Optionally print each matrix to verify (can remove if not needed)\n",
    "    print(f\"Decoding Accuracy Matrix for entry {idx + 1}:\")\n",
    "    print(accuracy_df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-01: matrices saved in 'accuracy_matrices.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~~ Save the list of accuracy matrices\n",
    "save_folder = f\"/u/kazma/MINT/data/adult/processed/{modality}/{subject}\"\n",
    "save_path = os.path.join(save_folder, \"accuracy_matrices.pkl\") #  a pickle file\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(all_accuracy_matrices, f)\n",
    "print(f\"{subject}: matrices saved in 'accuracy_matrices.pkl'\")\n",
    "# ~~~~~~~~~~~~~~~~ Save the list of accuracy matrices ~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
