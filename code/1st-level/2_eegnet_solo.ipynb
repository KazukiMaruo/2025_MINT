{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Libraries\n",
    "import sys, os\n",
    "import mne # Python package for processing and analyzing electrophysiological data\n",
    "import numpy as np\n",
    "from glob import glob # look for all the pathnames matching a specified pattern according to the rules\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA # ICA (Independent Component Analysis) algorithm, which is for artifact removal\n",
    "from autoreject import AutoReject # Python package for automatically rejecting bad epochs in EEG/MEG data\n",
    "import json\n",
    "import owncloud\n",
    "import pandas as pd\n",
    "import braindecode\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, cross_validate, StratifiedKFold\n",
    "\n",
    "# Deep learning\n",
    "from braindecode.models import EEGNetv4\n",
    "from braindecode.preprocessing import exponential_moving_standardize\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# ~~~~~~~~~~~~~~ Libraries ~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/viper/u/kazma/MINT/code\n",
      "visual data of sub-05 is processed\n",
      "condition pair: totaldot\n",
      "EEGNET parameters:\n",
      "====== CV: 10\n",
      "====== Max epochs: 200\n",
      "====== Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~ Parameters\n",
    "group = 'adult'\n",
    "modality = 'visual' # 'visual' or 'audio'\n",
    "subject = 'sub-05'\n",
    "condition = 'totaldot' # 'singledot', 'totaldot', 'circum'\n",
    "\n",
    "# EEGNET parameters\n",
    "# Load the JSON file\n",
    "code_directory = os.path.dirname(\"/u/kazma/MINT/code/1st-level\")\n",
    "sys.path.append(code_directory) \n",
    "os.chdir(os.path.dirname(\"/u/kazma/MINT/code/1st-level\"))\n",
    "print(os.getcwd())\n",
    "with open(\"config.json\") as f: # import variables from config.json\n",
    "    config = json.load(f) \n",
    "globals().update(config)\n",
    "\n",
    "\n",
    "# Print out each parameter\n",
    "print(f\"{modality} data of {subject} is processed\")\n",
    "print(f\"condition pair: {condition}\")\n",
    "print(\"EEGNET parameters:\")\n",
    "print(f\"====== CV: {EEGNET_CV}\")\n",
    "print(f\"====== Max epochs: {EEGNET_MAX_EPOCHS}\")\n",
    "print(f\"====== Batch size: {EEGNET_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /ptmp/kazma/DATA-MINT/data/adult/interim/visual/sub-05/ses-01/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "1260 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /ptmp/kazma/DATA-MINT/data/adult/interim/visual/sub-05/ses-02/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "1260 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /ptmp/kazma/DATA-MINT/data/adult/interim/visual/sub-05/ses-03/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "1260 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "3780 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2029821/305156617.py:19: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs([eegdata_dict['ses-01'], eegdata_dict['ses-02'], eegdata_dict['ses-03']])\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~ Set the working directory\n",
    "path = f\"{COMPUTE_DIR}/data/{group}/interim/{modality}\"\n",
    "sub_folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "sub_folders_sorted = sorted(sub_folders, key=lambda x: int(re.search(r'\\d+', x).group())) # Sort the folders based on the numeric part after \"sub-\"\n",
    "# ~~~~~~~~~~~~~~ Set the working directory ~~~~~~~~~~~~~~\n",
    "\n",
    "# ~~~~~~~~~~~~~~ Concatanating 3 sessions\n",
    "each_sub_path = f\"{COMPUTE_DIR}/data/{group}/interim/{modality}/{subject}\"\n",
    "each_sub_folders = [f for f in os.listdir(each_sub_path) if os.path.isdir(os.path.join(each_sub_path, f))]\n",
    "each_sub_folders_sorted = sorted(each_sub_folders, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "eegdata_dict = {}\n",
    "for x,ses_loop in enumerate(each_sub_folders_sorted):\n",
    "    sub_filename = os.path.join(path, subject, ses_loop, 'epochs-epo.fif') \n",
    "    epochs = mne.read_epochs(sub_filename, preload=True)\n",
    "    eegdata_dict[ses_loop] = epochs\n",
    "\n",
    "# concatanate all 3 sessions into 1 epoch\n",
    "epochs = mne.concatenate_epochs([eegdata_dict['ses-01'], eegdata_dict['ses-02'], eegdata_dict['ses-03']])\n",
    "# ~~~~~~~~~~~~~~ Concatanating 3 sessions ~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected trials: 1260\n",
      "Target condition: totaldot\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~ Concatanating 3 sessions csv file\n",
    "each_sub_path = f\"{COMPUTE_DIR}/data/{group}/raw/{modality}/{subject}\"\n",
    "each_sub_folders = [f for f in os.listdir(each_sub_path) if os.path.isdir(os.path.join(each_sub_path, f))]\n",
    "each_sub_folders_sorted = sorted(each_sub_folders, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "csv_dict = {}\n",
    "for x,ses_loop in enumerate(each_sub_folders_sorted):\n",
    "    csv_file = f\"{subject}_{modality}_{ses_loop}.csv\"\n",
    "    sub_filename = os.path.join(each_sub_path, ses_loop, csv_file) \n",
    "    csv_df = pd.read_csv(sub_filename)\n",
    "    csv_dict[ses_loop] = csv_df\n",
    "\n",
    "# concatanate all 3 sessions into 1 epoch\n",
    "csv_dfs = pd.concat([csv_dict['ses-01'], csv_dict['ses-02'], csv_dict['ses-03']], axis=0).reset_index(drop=True)\n",
    "# ~~~~~~~~~~~~~~ Concatanating 3 sessions csv file~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~ Obtain target conditions' trials\n",
    "csv_dfs['Extracted'] = csv_dfs['condition'].str.split('_').str[-1]\n",
    "csv_idx = csv_dfs[['condition', 'Extracted']]\n",
    "condition_lists = csv_idx['Extracted'].unique()\n",
    "\n",
    "idx_to_keep = csv_dfs.index[csv_dfs['Extracted'] == condition].tolist()\n",
    "# idx_to_keep = csv_dfs.index[csv_dfs['Extracted'].isin(condition)].tolist()\n",
    "\n",
    "selected_epochs = epochs[idx_to_keep]\n",
    "print(f\"Selected trials: {len(selected_epochs)}\")\n",
    "print(f\"Target condition: {condition}\")\n",
    "# ~~~~~~~~~~~~~~ Obtain target conditions' trials ~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "=====================================================\n",
      "=====================================================\n",
      " Condition lists: ['numerosity 1', 'numerosity 2', 'numerosity 3', 'numerosity 4', 'numerosity 5', 'numerosity 6']\n",
      " Total trials: 1260\n",
      " Time points: 251\n",
      " Time window (ms): 0.0 - 500.0\n"
     ]
    }
   ],
   "source": [
    "# Crop epochs to the desired time range\n",
    "cropped_epochs = selected_epochs.copy().crop(tmin=EEGNET_MIN_TIME, tmax=EEGNET_MAX_TIME)\n",
    "\n",
    "# Get the info about the cropped data\n",
    "conditions = list(cropped_epochs.event_id.keys()) # list of conditions\n",
    "n_conditions = len(conditions) # number of conditions\n",
    "n_trials = len(cropped_epochs) # number of trials\n",
    "n_samples = cropped_epochs.get_data().shape[2]\n",
    "n_channels = cropped_epochs.get_data().shape[1]\n",
    "min_time = cropped_epochs.times[0]*1000   # First time point in milli seconds\n",
    "max_time = cropped_epochs.times[-1]*1000    # Last time point in milli seconds\n",
    "\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"=====================================================\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(f\" Condition lists: {conditions}\")\n",
    "print(f\" Total trials: {n_trials}\")\n",
    "print(f\" Time points: {n_samples}\")\n",
    "print(f\" Time window (ms): {min_time} - {max_time}\")\n",
    "\n",
    "\n",
    "# check GPU availability\n",
    "cuda = torch.cuda.is_available()  # Check if a GPU is available\n",
    "device = \"cuda\" if cuda else \"cpu\"  # Use \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuning for performance\n",
    "\n",
    "\n",
    "# Set random seeds\n",
    "# PURPOSE: reproducibility across runs, random initializations (e.g., model weights) yield consistant results\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "\n",
    "# dictionaries for decoding accuracy\n",
    "pairwise_decoding_accuracies = {}\n",
    "# dictionaries for decoding accuracy of standard deviation\n",
    "pairwise_decoding_accuracies_std = {}\n",
    "# dictionaries for estimators\n",
    "pairwise_decoding_estimators = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerosity 1 vs. numerosity 2\n",
      "Trials per condition:\n",
      "{'numerosity 1': 213, 'numerosity 2': 213}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7446\u001b[0m  1.8924\n",
      "      2        \u001b[36m0.6940\u001b[0m  1.8126\n",
      "      3        \u001b[36m0.6865\u001b[0m  1.8027\n",
      "      4        \u001b[36m0.6326\u001b[0m  1.8872\n",
      "      5        \u001b[36m0.6262\u001b[0m  2.3999\n",
      "      6        0.6265  2.1083\n",
      "      7        0.6378  2.2816\n",
      "      8        0.6302  2.2017\n",
      "      9        \u001b[36m0.5829\u001b[0m  2.2011\n",
      "     10        0.5910  2.5109\n",
      "     11        0.5981  2.5915\n",
      "     12        0.5863  2.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        0.5888  2.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/viper/ptmp/kazma/miniforge3/envs/MINT/lib/python3.9/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8202\u001b[0m  2.5893\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7829\u001b[0m  2.5986\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7731\u001b[0m  2.6169\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8134\u001b[0m  2.6115\n",
      "     14        \u001b[36m0.5801\u001b[0m  2.6109\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7357\u001b[0m  2.7992\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8059\u001b[0m  2.6969\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7905\u001b[0m  2.6951\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7595\u001b[0m  2.7866\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7601\u001b[0m  2.9049\n",
      "      2        \u001b[36m0.7179\u001b[0m  2.4988\n",
      "      2        \u001b[36m0.7011\u001b[0m  2.5077\n",
      "      2        \u001b[36m0.7035\u001b[0m  2.6061\n",
      "      2        \u001b[36m0.7300\u001b[0m  2.6026\n",
      "     15        \u001b[36m0.5696\u001b[0m  2.6009\n",
      "      2        \u001b[36m0.7210\u001b[0m  2.6065\n",
      "      2        \u001b[36m0.6906\u001b[0m  2.9051\n",
      "      2        \u001b[36m0.7129\u001b[0m  2.6938\n",
      "      2        \u001b[36m0.6770\u001b[0m  2.7113\n",
      "      3        \u001b[36m0.6824\u001b[0m  2.4095\n",
      "      2        \u001b[36m0.7235\u001b[0m  2.8060\n",
      "      3        \u001b[36m0.6980\u001b[0m  2.5926\n",
      "      3        \u001b[36m0.7000\u001b[0m  2.6043\n",
      "      3        \u001b[36m0.7266\u001b[0m  2.6894\n",
      "     16        0.5902  2.5987\n",
      "      3        \u001b[36m0.6713\u001b[0m  2.5912\n",
      "      3        \u001b[36m0.6815\u001b[0m  2.6062\n",
      "      3        \u001b[36m0.6981\u001b[0m  2.6120\n",
      "      4        \u001b[36m0.6523\u001b[0m  2.4968\n",
      "      3        \u001b[36m0.6767\u001b[0m  2.6869\n",
      "      3        \u001b[36m0.6744\u001b[0m  2.8019\n",
      "      4        0.6993  2.5964\n",
      "      4        \u001b[36m0.6701\u001b[0m  2.7060\n",
      "      4        \u001b[36m0.6743\u001b[0m  2.6007\n",
      "     17        0.5895  2.6932\n",
      "      4        \u001b[36m0.6672\u001b[0m  2.6833\n",
      "      4        \u001b[36m0.6462\u001b[0m  2.6963\n",
      "      4        \u001b[36m0.6793\u001b[0m  2.6967\n",
      "      5        0.6543  2.4973\n",
      "      4        \u001b[36m0.6475\u001b[0m  2.8100\n",
      "      5        \u001b[36m0.6481\u001b[0m  2.5107\n",
      "      4        \u001b[36m0.6588\u001b[0m  2.8964\n",
      "      5        \u001b[36m0.6465\u001b[0m  2.6855\n",
      "      5        0.6785  2.5999\n",
      "     18        0.5758  2.6912\n",
      "      5        \u001b[36m0.6631\u001b[0m  2.6051\n",
      "      5        0.6506  2.6089\n",
      "      6        \u001b[36m0.6204\u001b[0m  2.4091\n",
      "      5        \u001b[36m0.6532\u001b[0m  2.6954\n",
      "      6        0.6834  2.5921\n",
      "      5        \u001b[36m0.6423\u001b[0m  2.9002\n",
      "      5        \u001b[36m0.6448\u001b[0m  2.8906\n",
      "      6        0.6574  2.5117\n",
      "      6        \u001b[36m0.6582\u001b[0m  2.6078\n",
      "     19        \u001b[36m0.5608\u001b[0m  2.6030\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_conditions):\n",
    "    for j in range(i + 1, n_conditions):\n",
    "\n",
    "            cond1 = conditions[i]\n",
    "            cond2 = conditions[j]\n",
    "\n",
    "            print(f\"{cond1} vs. {cond2}\")\n",
    "            \n",
    "            filtered_epochs = cropped_epochs[cond1,cond2]\n",
    "            a1 = {cond: (filtered_epochs.events[:, 2] == event_id).sum() \n",
    "                for cond, event_id in filtered_epochs.event_id.items()}\n",
    "\n",
    "            # Print the number of trials per condition\n",
    "            print(f\"Trials per condition:\\n{a1}\")\n",
    "\n",
    "            # get EEG data\n",
    "            X = filtered_epochs.get_data(picks=mne.pick_types(filtered_epochs.info, eeg=True, eog=False, exclude='bads'))\n",
    "\n",
    "            # get labels (=numerosity)\n",
    "            unique_labels = np.unique(filtered_epochs.events[:,-1])\n",
    "            label_0, label_1 = unique_labels[0], unique_labels[1]  # Assign the first label to 0 and the second to 1\n",
    "\n",
    "            y = np.where(filtered_epochs.events[:, -1] == label_0, 0, 1)\n",
    "\n",
    "            skfold = StratifiedKFold(n_splits=EEGNET_CV, shuffle=True, random_state=23)\n",
    "\n",
    "            # exp. moving std. for each trial\n",
    "            for s in range(X.shape[0]):\n",
    "                X[s,:,:] = exponential_moving_standardize(X[s,:,:], factor_new=0.001, init_block_size=None, eps=1e-4)\n",
    "\n",
    "            # create the model\n",
    "            net = EEGClassifier(\n",
    "                \"EEGNetv4\", \n",
    "                            module__n_chans=n_channels, # Number of EEG channels\n",
    "                            module__n_outputs=2,               # Number of outputs of the model. This is the number of classes in the case of classification.\n",
    "                            module__n_times=n_samples,         # Number of time samples of the input window.\n",
    "                            module__final_conv_length='auto',  # Length of the final convolution layer. If \"auto\", it is set based on the n_times.\n",
    "                            module__pool_mode='mean',          # Pooling method to use in pooling layers\n",
    "                            module__F1=8,                      # Number of temporal filters in the first convolutional layer.\n",
    "                            module__D=2,                       # Depth multiplier for the depthwise convolution.\n",
    "                            module__F2=16,                     # Number of pointwise filters in the separable convolution. Usually set to ``F1 * D``.\n",
    "                            module__kernel_length=64,         # Length of the temporal convolution kernel. Usally sampling rate / 2 = 500/2 = 250\n",
    "                            module__third_kernel_size=(8, 4), \n",
    "                            module__drop_prob=0.25,            # Dropout probability after the second conv block and before the last layer. 0.5 for within-subject classification, 0.25 in cross-subject clasification\n",
    "                            module__chs_info=None,             # (list of dict) – Information about each individual EEG channel. This should be filled with info[\"chs\"]. Refer to mne.Info for more details.\n",
    "                            module__input_window_seconds=None, # Length of the input window in seconds.\n",
    "                            module__sfreq=SFREQ,\n",
    "                            max_epochs=EEGNET_MAX_EPOCHS,\n",
    "                            batch_size=EEGNET_BATCH_SIZE,\n",
    "                            train_split=None,\n",
    "            )\n",
    "\n",
    "\n",
    "            cvs = cross_validate(net, \n",
    "                                X, \n",
    "                                y, \n",
    "                                scoring=\"accuracy\", # for balanced classes, this corresponds to accuracy,\n",
    "                                # chance level might be 0 (adjusted = False), or 0.X (adjusted = True)\n",
    "                                cv=skfold, \n",
    "                                n_jobs=-1, # only 1 to avoid overload in parallel jobs, in non-par jobs it could be -1\n",
    "                                return_estimator=True, # if you need the model to estimate on another test set\n",
    "                                return_train_score=False,\n",
    "                                )\n",
    "            \n",
    "            pairwise_decoding_accuracies[(cond1,cond2)] = np.mean(cvs['test_score'])\n",
    "            pairwise_decoding_accuracies_std[(cond1,cond2)] = np.std(cvs['test_score'])\n",
    "            pairwise_decoding_estimators[(cond1,cond2)] = cvs['estimator']\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy\n",
    "save_folder = f\"{COMPUTE_DIR}/data/{group}/processed/{modality}/{subject}\"\n",
    "\n",
    "# concatenated_str = \"_\".join(condition)\n",
    "filename = f\"EEGNet_accuracy_{condition}.pkl\"\n",
    "save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(pairwise_decoding_accuracies, f)\n",
    "print(f\"{subject}: saved in {filename}\")\n",
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy ~~~~~~~~~~~~~~~~\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation\n",
    "filename = f\"EEGNet_std_{condition}.pkl\"\n",
    "save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(pairwise_decoding_accuracies_std, f)\n",
    "print(f\"{subject}: saved in {filename}\")\n",
    "# ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation ~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~ Save the EEGNet estimation\n",
    "filename = f\"EEGNet_estimator_{condition}.pkl\"\n",
    "save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(pairwise_decoding_estimators, f)\n",
    "print(f\"{subject}: saved in {filename}\")\n",
    "# ~~~~~~~~~~~~~~~~ Save the EEGNet estimation ~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
