{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Libraries\n",
    "import sys, os\n",
    "import mne # Python package for processing and analyzing electrophysiological data\n",
    "import numpy as np\n",
    "from glob import glob # look for all the pathnames matching a specified pattern according to the rules\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA # ICA (Independent Component Analysis) algorithm, which is for artifact removal\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import permutation_test\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average time series for one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the working directory\n",
    "path = \"/u/kazma/MINT/data/interim/visual\"\n",
    "sub_folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "sub_folders_sorted = sorted(sub_folders, key=lambda x: int(re.search(r'\\d+', x).group())) # Sort the folders based on the numeric part after \"sub-\"\n",
    "\n",
    "\n",
    "\n",
    "for sub_loop in sub_folders_sorted:\n",
    "\n",
    "    # subject folder \n",
    "    sub_filename = os.path.join(path, sub_loop, 'RDM_epochs-epo.fif')\n",
    "\n",
    "    # Load epochs\n",
    "    epochs = mne.read_epochs(sub_filename, preload=True)\n",
    "    print(f\"{sub_loop} is analyzed\")\n",
    "\n",
    "    eeg_channels = epochs.pick_types(eeg=True).ch_names\n",
    "    print(\"EEG Channels:\", eeg_channels)\n",
    "    print(\"Number of EEG Channels:\", len(eeg_channels))\n",
    "    \n",
    "    # Get the condition names\n",
    "    condition_names = list(epochs.event_id.keys())\n",
    "    evoked_list = []\n",
    "\n",
    "    # Compute the averaged epoch for each condition and each channel\n",
    "    for condition in condition_names:\n",
    "        evoked = epochs[condition].average()  # Average the epochs for the condition\n",
    "        evoked_list.append(evoked)\n",
    "\n",
    "    # Store 6 vectors, representing averaged vector for each condition\n",
    "    averaged_data = {}\n",
    "\n",
    "    # Specify the channel to plot (e.g., channel name 'Cz')\n",
    "    channel_name = 'Fz'  # Replace with the channel of your choice\n",
    "\n",
    "    # Figure\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Define colors for the conditions\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 6))\n",
    "\n",
    "    for i, evoked in enumerate(evoked_list):\n",
    "        # Extract data for the specified channel\n",
    "        channel_index = evoked.ch_names.index(channel_name)  # Get index of the channel\n",
    "        channel_data = evoked.data[channel_index]  # Get the data for the specific channel\n",
    "        times = evoked.times * 1000  # Convert time to milliseconds\n",
    "\n",
    "        # Store the data\n",
    "        averaged_data[condition_names[i]] = channel_data\n",
    "        \n",
    "        # Plot the channel data\n",
    "        plt.plot(times, channel_data, linewidth=3, alpha=0.5) \n",
    "        plt.plot(times, channel_data, label=condition_names[i], linewidth=2)  # Plot time series\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Average Time Series for Channel: ' + channel_name)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Amplitude (ÂµV)')\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=0.5)  # Add a horizontal line at y=0\n",
    "    plt.axvline(0, color='black', linestyle='-', linewidth=1)  # Add a vertical line at t=0\n",
    "    plt.xlim(-20, 900)  # Set x-axis limits\n",
    "    # plt.ylim(-2.0e-8, 2.0e-8)  # Set y-axis limits\n",
    "    # plt.legend(ncol=3, loc='lower left', framealpha=1)\n",
    "\n",
    "    # Set the box line color to gray\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_color('gray')\n",
    "    ax.spines['right'].set_color('gray')\n",
    "    ax.spines['bottom'].set_color('gray')\n",
    "    ax.spines['left'].set_color('gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "path = \"/u/kazma/MINT/data/interim/visual\"\n",
    "sub_folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "sub_folders_sorted = sorted(sub_folders, key=lambda x: int(re.search(r'\\d+', x).group())) # Sort the folders based on the numeric part after \"sub-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUB_LOOP\n",
    "# for subject in sub_folders_sorted\n",
    "\n",
    "# subject folder \n",
    "sub_filename = os.path.join(path, 'sub-01_ses-01', 'RDM_epochs-epo.fif')\n",
    "\n",
    "# Load epochs\n",
    "epochs = mne.read_epochs(sub_filename, preload=True)\n",
    "\n",
    "# Get the information about the data\n",
    "conditions = list(epochs.event_id.keys())\n",
    "n_conditions = len(conditions)\n",
    "# n_trials = len(epochs)\n",
    "n_channels = epochs.get_data().shape[1]\n",
    "\n",
    "\n",
    "n_samples = epochs.get_data().shape[2]\n",
    "min_time = epochs.times[0]*1000   # First time point in milli seconds\n",
    "max_time = epochs.times[-1]*1000    # Last time point in milli seconds\n",
    "\n",
    "\n",
    "# number of samples for each window\n",
    "window_size = 5 # 1 sample = 2ms, 5 samples = 10 ms\n",
    "\n",
    "df_dict = {}\n",
    "for condition in conditions:\n",
    "    x = epochs[condition]\n",
    "    x = x.get_data()\n",
    "    x = x[:,:,0:5]\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    df_dict[condition] = x\n",
    "\n",
    "\n",
    "# Define the order for conditions\n",
    "condition_order = {'singledot': 0, 'totaldot': 1, 'circum': 2}\n",
    "# Sort dictionary by first number and then by condition order\n",
    "sorted_condition_dict = dict(\n",
    "    sorted(\n",
    "        df_dict.items(),\n",
    "        key=lambda item: (\n",
    "            int(item[0].split('_')[0]),\n",
    "            condition_order.get(item[0].split('_')[2], 99),\n",
    "            int(item[0].split('_')[-1].split('.')[0])  # Sort by condition order, 99 as default for unmatched\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "pairwise_dissimilar_dict = {}\n",
    "\n",
    "# CONDITION_LOOP\n",
    "for i in range(n_conditions):\n",
    "    for j in range(i + 1, n_conditions):\n",
    "        \n",
    "        cond1 = list(sorted_condition_dict.keys())[i]\n",
    "        cond2 = list(sorted_condition_dict.keys())[j]\n",
    "        data_cond1 = sorted_condition_dict[cond1]\n",
    "        data_cond2 = sorted_condition_dict[cond2]\n",
    "        # Correlation\n",
    "        corr_matrix = np.corrcoef(data_cond1, data_cond2)\n",
    "        pairwise_dissimilar_dict[(cond1, cond2)] = 1 - np.abs(corr_matrix[0, 1])\n",
    "        print(f\"Correlation for {cond1} vs {cond2} in time points {window_size}: {np.abs(corr_matrix[0, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" # Window loop start\n",
    "for start in range(0, n_samples, window_size): # Loop over the epoch in steps of `window_size` to extract each 5-sample window\n",
    "    # Check if there are enough samples left for a full window\n",
    "    if start + window_size <= n_samples:\n",
    "\n",
    "        # Initialize the decoding dictionary with each condition containing a list of flattened sample windows\n",
    "        decoding_dict = {}\n",
    "\n",
    "        for condition in conditions:\n",
    "            x = epochs[condition] \n",
    "            x = x.get_data() # the shape: (n_trials, n_channels, n_times)\n",
    "            x = x[:,:,start:start + window_size] # 5 sample is a window size\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            decoding_dict[condition] = x\n",
    "\n",
    "\n",
    "        # Dictionary to store decoding results for each pair\n",
    "        pairwise_decoding_accuracies = {}\n",
    "\n",
    "        for i in range(n_conditions):\n",
    "            for j in range(i + 1, n_conditions):\n",
    "\n",
    "                cond1 = conditions[i]\n",
    "                cond2 = conditions[j]\n",
    "                    \n",
    "                # Classification\n",
    "                data_cond1 = decoding_dict[cond1]\n",
    "                data_cond2 = decoding_dict[cond2]\n",
    "                data = np.vstack((data_cond1, data_cond2))\n",
    "                labels = np.hstack((np.zeros(len(data_cond1)), np.ones(len(data_cond2))))\n",
    "\n",
    "                # Set cross-validation\n",
    "                cv = StratifiedKFold(n_splits=n_splits) # StratifiedKFold ensures that each fold has a proportional representation of both classes, so each fold maintains a 50:50 balance of numerosity 1 and numerosity 2 trials.\n",
    "\n",
    "                # Time-resolved decoding storage\n",
    "                decoding_accuracies = []\n",
    "\n",
    "                # Cross-validation for this time point\n",
    "                for train_idx, test_idx in cv.split(data, labels):\n",
    "                    X_train, X_test = data[train_idx], data[test_idx] # the EEG data for training and testing.\n",
    "                    y_train, y_test = labels[train_idx], labels[test_idx] # corresponding labels for the training and testing data.\n",
    "                    \n",
    "                    clf = SVC(kernel='linear') # initializes a support vector machine (SVM) classifier with a linear kernel.\n",
    "                    clf.fit(X_train, y_train) # trains the classifier on the training data (X_train) with labels (y_train).\n",
    "                    y_pred = clf.predict(X_test) \n",
    "                    decoding_accuracies.append(accuracy_score(y_test, y_pred)) # calculates the classification accuracy by comparing the true labels (y_test) with the predicted labels (y_pred).\n",
    "\n",
    "                avg_accuracy = np.mean(decoding_accuracies)\n",
    "                # Store the average accuracy for the condition pair\n",
    "                pairwise_decoding_accuracies[(cond1, cond2)] = avg_accuracy\n",
    "\n",
    "                print(f\"Average accuracy for {cond1} vs {cond2} in time points {start + window_size}: {avg_accuracy:.2f}\")\n",
    "\n",
    "\n",
    "    # Average accuracy for this time point across CV folds\n",
    "    all_decoding_accuracy.append(pairwise_decoding_accuracies)\n",
    "    print(f\"Time point {start + window_size} is done\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
