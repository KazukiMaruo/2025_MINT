{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ Libraries\n",
    "import sys, os\n",
    "import mne # Python package for processing and analyzing electrophysiological data\n",
    "import numpy as np\n",
    "from glob import glob # look for all the pathnames matching a specified pattern according to the rules\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing import ICA # ICA (Independent Component Analysis) algorithm, which is for artifact removal\n",
    "from autoreject import AutoReject # Python package for automatically rejecting bad epochs in EEG/MEG data\n",
    "import json\n",
    "import owncloud\n",
    "import pandas as pd\n",
    "import braindecode\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, cross_validate, StratifiedKFold\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Deep learning\n",
    "from braindecode.models import EEGNetv4\n",
    "from braindecode.preprocessing import exponential_moving_standardize\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# ~~~~~~~~~~~~~~ Libraries ~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/viper/u/kazma/MINT/code\n",
      "visual data of baby: sub-01 is processed\n",
      "EEGNET parameters:\n",
      "====== CV: 10\n",
      "====== Max epochs: 200\n",
      "====== Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~ Parameters\n",
    "group = 'baby'\n",
    "modality = 'visual' # 'visual' or 'audio'\n",
    "subject = 'sub-01'\n",
    "analysis = 'condition-wise' # 'all' or 'condition-wise'\n",
    "\n",
    "# EEGNET parameters\n",
    "# Load the JSON file\n",
    "code_directory = os.path.dirname(\"/u/kazma/MINT/code/1st-level\")\n",
    "sys.path.append(code_directory) \n",
    "os.chdir(os.path.dirname(\"/u/kazma/MINT/code/1st-level\"))\n",
    "print(os.getcwd())\n",
    "with open(\"config.json\") as f: # import variables from config.json\n",
    "    config = json.load(f) \n",
    "globals().update(config)\n",
    "\n",
    "\n",
    "# Print out each parameter\n",
    "print(f\"{modality} data of {group}: {subject} is processed\")\n",
    "print(\"EEGNET parameters:\")\n",
    "print(f\"====== CV: {EEGNET_CV}\")\n",
    "print(f\"====== Max epochs: {EEGNET_MAX_EPOCHS}\")\n",
    "print(f\"====== Batch size: {EEGNET_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /ptmp/kazma/DATA-MINT/data/baby/interim/visual/sub-01/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "847 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ~~~~~~~~~~~~~~ read 1 session\n",
    "each_sub_path = f\"{COMPUTE_DIR}/data/{group}/interim/{modality}/{subject}\"\n",
    "each_sub_folders = [f for f in os.listdir(each_sub_path) if os.path.isdir(os.path.join(each_sub_path, f))]\n",
    "each_sub_folders_sorted = sorted(each_sub_folders, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "sub_filename = os.path.join(each_sub_path, 'epochs-epo.fif') \n",
    "epochs = mne.read_epochs(sub_filename, preload=True)\n",
    "\n",
    "# ~~~~~~~~~~~~~~ read 1 session ~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~ read 1 session csv file\n",
    "each_sub_path = f\"{COMPUTE_DIR}/data/{group}/raw/{modality}/{subject}\"\n",
    "\n",
    "csv_file = f\"{subject}_ses-01_{modality}.csv\"\n",
    "sub_filename = os.path.join(each_sub_path, csv_file) \n",
    "csv_dfs = pd.read_csv(sub_filename)\n",
    "# ~~~~~~~~~~~~~~ read 1 session csv file ~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~ Obtain target conditions' trials\n",
    "csv_dfs['Extracted'] = csv_dfs['condition'].str.split('_').str[-1]\n",
    "csv_dfs = csv_dfs[csv_dfs[\"Attention\"]!= 0].reset_index(drop=True)\n",
    "\n",
    "csv_idx = csv_dfs[['condition', 'Extracted', 'Attention']]\n",
    "condition_lists = csv_idx['Extracted'].unique()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis == 'all':\n",
    "    cropped_epochs = epochs.copy().crop(tmin=EEGNET_MIN_TIME, tmax=EEGNET_MAX_TIME)\n",
    "\n",
    "    # Get the info about the cropped data\n",
    "    conditions = list(cropped_epochs.event_id.keys()) # list of conditions\n",
    "    n_conditions = len(conditions) # number of conditions\n",
    "    n_trials = len(cropped_epochs) # number of trials\n",
    "    n_samples = cropped_epochs.get_data().shape[2]\n",
    "    n_channels = cropped_epochs.get_data().shape[1]\n",
    "    min_time = cropped_epochs.times[0]*1000   # First time point in milli seconds\n",
    "    max_time = cropped_epochs.times[-1]*1000    # Last time point in milli seconds\n",
    "\n",
    "\n",
    "    print(\"=====================================================\")\n",
    "    print(\"=====================================================\")\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    print(f\" Condition lists: {conditions}\")\n",
    "    print(f\" Total trials: {n_trials}\")\n",
    "    print(f\" Time points: {n_samples}\")\n",
    "    print(f\" Time window (ms): {min_time} - {max_time}\")\n",
    "\n",
    "\n",
    "    # check GPU availability\n",
    "    cuda = torch.cuda.is_available()  # Check if a GPU is available\n",
    "    device = \"cuda\" if cuda else \"cpu\"  # Use \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "\n",
    "    if cuda:\n",
    "        torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuning for performance\n",
    "\n",
    "\n",
    "    # Set random seeds\n",
    "    # PURPOSE: reproducibility across runs, random initializations (e.g., model weights) yield consistant results\n",
    "    seed = 20200220\n",
    "    set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "\n",
    "    # dictionaries for decoding accuracy\n",
    "    pairwise_decoding_accuracies = {}\n",
    "    # dictionaries for decoding accuracy of standard deviation\n",
    "    pairwise_decoding_accuracies_std = {}\n",
    "    # dictionaries for estimators\n",
    "    pairwise_decoding_estimators = {}\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_conditions):\n",
    "        for j in range(i + 1, n_conditions):\n",
    "\n",
    "            cond1 = conditions[i]\n",
    "            cond2 = conditions[j]\n",
    "\n",
    "            filtered_epochs = cropped_epochs[cond1,cond2]\n",
    "\n",
    "            # get EEG data\n",
    "            X = filtered_epochs.get_data(picks=mne.pick_types(filtered_epochs.info, eeg=True, eog=False, exclude='bads'))\n",
    "\n",
    "            # get labels (=numerosity)\n",
    "            unique_labels = np.unique(filtered_epochs.events[:,-1])\n",
    "            label_0, label_1 = unique_labels[0], unique_labels[1]  # Assign the first label to 0 and the second to 1\n",
    "\n",
    "            y = np.where(filtered_epochs.events[:, -1] == label_0, 0, 1)\n",
    "\n",
    "            counts = np.bincount(y)\n",
    "            print(f\"{cond1} vs {cond2}\")\n",
    "            print(f\"{counts[0]} vs. {counts[1]} trials\")\n",
    "\n",
    "            skfold = StratifiedKFold(n_splits=EEGNET_CV, shuffle=True, random_state=23)\n",
    "\n",
    "            # exp. moving std. for each trial\n",
    "            for s in range(X.shape[0]):\n",
    "                X[s,:,:] = exponential_moving_standardize(X[s,:,:], factor_new=0.001, init_block_size=None, eps=1e-4)\n",
    "\n",
    "            class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "\n",
    "            # create the model\n",
    "            net = EEGClassifier(\n",
    "                \"EEGNetv4\", \n",
    "                            module__n_chans=n_channels, # Number of EEG channels\n",
    "                            module__n_outputs=2,               # Number of outputs of the model. This is the number of classes in the case of classification.\n",
    "                            module__n_times=n_samples,         # Number of time samples of the input window.\n",
    "                            module__final_conv_length='auto',  # Length of the final convolution layer. If \"auto\", it is set based on the n_times.\n",
    "                            module__pool_mode='mean',          # Pooling method to use in pooling layers\n",
    "                            module__F1=8,                      # Number of temporal filters in the first convolutional layer.\n",
    "                            module__D=2,                       # Depth multiplier for the depthwise convolution.\n",
    "                            module__F2=16,                     # Number of pointwise filters in the separable convolution. Usually set to ``F1 * D``.\n",
    "                            module__kernel_length=64,         # Length of the temporal convolution kernel. Usally sampling rate / 2 = 500/2 = 250\n",
    "                            module__third_kernel_size=(8, 4), \n",
    "                            module__drop_prob=0.25,            # Dropout probability after the second conv block and before the last layer. 0.5 for within-subject classification, 0.25 in cross-subject clasification\n",
    "                            module__chs_info=None,             # (list of dict) – Information about each individual EEG channel. This should be filled with info[\"chs\"]. Refer to mne.Info for more details.\n",
    "                            module__input_window_seconds=None, # Length of the input window in seconds.\n",
    "                            module__sfreq=SFREQ,\n",
    "                            max_epochs=EEGNET_MAX_EPOCHS,\n",
    "                            batch_size=EEGNET_BATCH_SIZE,\n",
    "                            train_split=None,\n",
    "                            criterion__weight=torch.Tensor(class_weights).to('cpu'), # class weight\n",
    "            )\n",
    "            \n",
    "            cvs = cross_validate(net, \n",
    "                                X, \n",
    "                                y, \n",
    "                                scoring=\"balanced_accuracy\", # for balanced classes, this corresponds to accuracy,\n",
    "                                # chance level might be 0 (adjusted = False), or 0.X (adjusted = True)\n",
    "                                cv=skfold, \n",
    "                                n_jobs=-1, # only 1 to avoid overload in parallel jobs, in non-par jobs it could be -1\n",
    "                                return_estimator=True, # if you need the model to estimate on another test set\n",
    "                                return_train_score=False,\n",
    "                                )\n",
    "                        \n",
    "            pairwise_decoding_accuracies[(cond1,cond2)] = np.mean(cvs['test_score'])\n",
    "            pairwise_decoding_accuracies_std[(cond1,cond2)] = np.std(cvs['test_score'])\n",
    "            pairwise_decoding_estimators[(cond1,cond2)] = cvs['estimator']\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~ Save the decoding accuracy\n",
    "    save_folder = f\"{COMPUTE_DIR}/data/{group}/processed/{modality}/{subject}\"\n",
    "\n",
    "    filename = f\"EEGNet_accuracy_pairwise_{analysis}_timewindow350.pkl\"\n",
    "    save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(pairwise_decoding_accuracies, f)\n",
    "    print(f\"{subject}: saved in {filename}\")\n",
    "    # ~~~~~~~~~~~~~~~~ Save the decoding accuracy ~~~~~~~~~~~~~~~~\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation\n",
    "    filename = f\"EEGNet_std_pairwise_{analysis}_timewindow350.pkl\"\n",
    "    save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(pairwise_decoding_accuracies_std, f)\n",
    "    print(f\"{subject}: saved in {filename}\")\n",
    "    # ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation ~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~ Save the EEGNet estimation\n",
    "    filename = f\"EEGNet_estimator_pairwise_{analysis}_timewindow350.pkl\"\n",
    "    save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(pairwise_decoding_estimators, f)\n",
    "    print(f\"{subject}: saved in {filename}\")\n",
    "    # ~~~~~~~~~~~~~~~~ Save the EEGNet estimation ~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "else:\n",
    "    for target_condition in condition_lists:\n",
    "\n",
    "        idx_to_keep = csv_dfs.index[csv_dfs['Extracted'] == target_condition].tolist()\n",
    "\n",
    "        selected_epochs = epochs[idx_to_keep]\n",
    "        print(f\"Selected trials: {len(selected_epochs)}\")\n",
    "        print(f\"Target condition: {target_condition}\") \n",
    "        # ~~~~~~~~~~~~~~ Obtain target conditions' trials ~~~~~~~~~~~~~~\n",
    "        # Crop epochs to the desired time range\n",
    "        cropped_epochs = selected_epochs.copy().crop(tmin=EEGNET_MIN_TIME, tmax=EEGNET_MAX_TIME)\n",
    "\n",
    "        # Get the info about the cropped data\n",
    "        conditions = list(cropped_epochs.event_id.keys()) # list of conditions\n",
    "        n_conditions = len(conditions) # number of conditions\n",
    "        n_trials = len(cropped_epochs) # number of trials\n",
    "        n_samples = cropped_epochs.get_data().shape[2]\n",
    "        n_channels = cropped_epochs.get_data().shape[1]\n",
    "        min_time = cropped_epochs.times[0]*1000   # First time point in milli seconds\n",
    "        max_time = cropped_epochs.times[-1]*1000    # Last time point in milli seconds\n",
    "\n",
    "\n",
    "        print(\"=====================================================\")\n",
    "        print(\"=====================================================\")\n",
    "        print(\"=====================================================\")\n",
    "\n",
    "        print(f\" Condition lists: {conditions}\")\n",
    "        print(f\" Total trials: {n_trials}\")\n",
    "        print(f\" Time points: {n_samples}\")\n",
    "        print(f\" Time window (ms): {min_time} - {max_time}\")\n",
    "\n",
    "\n",
    "        # check GPU availability\n",
    "        cuda = torch.cuda.is_available()  # Check if a GPU is available\n",
    "        device = \"cuda\" if cuda else \"cpu\"  # Use \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "\n",
    "        if cuda:\n",
    "            torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuning for performance\n",
    "\n",
    "\n",
    "        # Set random seeds\n",
    "        # PURPOSE: reproducibility across runs, random initializations (e.g., model weights) yield consistant results\n",
    "        seed = 20200220\n",
    "        set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "\n",
    "        # dictionaries for decoding accuracy\n",
    "        pairwise_decoding_accuracies = {}\n",
    "        # dictionaries for decoding accuracy of standard deviation\n",
    "        pairwise_decoding_accuracies_std = {}\n",
    "        # dictionaries for estimators\n",
    "        pairwise_decoding_estimators = {}\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(n_conditions):\n",
    "            for j in range(i + 1, n_conditions):\n",
    "\n",
    "                cond1 = conditions[i]\n",
    "                cond2 = conditions[j]\n",
    "\n",
    "                filtered_epochs = cropped_epochs[cond1,cond2]\n",
    "\n",
    "                # get EEG data\n",
    "                X = filtered_epochs.get_data(picks=mne.pick_types(filtered_epochs.info, eeg=True, eog=False, exclude='bads'))\n",
    "\n",
    "                # get labels (=numerosity)\n",
    "                unique_labels = np.unique(filtered_epochs.events[:,-1])\n",
    "                label_0, label_1 = unique_labels[0], unique_labels[1]  # Assign the first label to 0 and the second to 1\n",
    "\n",
    "                y = np.where(filtered_epochs.events[:, -1] == label_0, 0, 1)\n",
    "\n",
    "                counts = np.bincount(y)\n",
    "                print(f\"{cond1} vs {cond2}\")\n",
    "                print(f\"{counts[0]} vs. {counts[1]} trials\")\n",
    "\n",
    "                skfold = StratifiedKFold(n_splits=EEGNET_CV, shuffle=True, random_state=23)\n",
    "\n",
    "                # exp. moving std. for each trial\n",
    "                for s in range(X.shape[0]):\n",
    "                    X[s,:,:] = exponential_moving_standardize(X[s,:,:], factor_new=0.001, init_block_size=None, eps=1e-4)\n",
    "\n",
    "                class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "\n",
    "                # create the model\n",
    "                net = EEGClassifier(\n",
    "                    \"EEGNetv4\", \n",
    "                                module__n_chans=n_channels, # Number of EEG channels\n",
    "                                module__n_outputs=2,               # Number of outputs of the model. This is the number of classes in the case of classification.\n",
    "                                module__n_times=n_samples,         # Number of time samples of the input window.\n",
    "                                module__final_conv_length='auto',  # Length of the final convolution layer. If \"auto\", it is set based on the n_times.\n",
    "                                module__pool_mode='mean',          # Pooling method to use in pooling layers\n",
    "                                module__F1=8,                      # Number of temporal filters in the first convolutional layer.\n",
    "                                module__D=2,                       # Depth multiplier for the depthwise convolution.\n",
    "                                module__F2=16,                     # Number of pointwise filters in the separable convolution. Usually set to ``F1 * D``.\n",
    "                                module__kernel_length=64,         # Length of the temporal convolution kernel. Usally sampling rate / 2 = 500/2 = 250\n",
    "                                module__third_kernel_size=(8, 4), \n",
    "                                module__drop_prob=0.25,            # Dropout probability after the second conv block and before the last layer. 0.5 for within-subject classification, 0.25 in cross-subject clasification\n",
    "                                module__chs_info=None,             # (list of dict) – Information about each individual EEG channel. This should be filled with info[\"chs\"]. Refer to mne.Info for more details.\n",
    "                                module__input_window_seconds=None, # Length of the input window in seconds.\n",
    "                                module__sfreq=SFREQ,\n",
    "                                max_epochs=EEGNET_MAX_EPOCHS,\n",
    "                                batch_size=EEGNET_BATCH_SIZE,\n",
    "                                train_split=None,\n",
    "                                criterion__weight=torch.Tensor(class_weights).to('cpu'), # class weight\n",
    "                )\n",
    "                \n",
    "                cvs = cross_validate(net, \n",
    "                                    X, \n",
    "                                    y, \n",
    "                                    scoring=\"balanced_accuracy\", # for balanced classes, this corresponds to accuracy,\n",
    "                                    # chance level might be 0 (adjusted = False), or 0.X (adjusted = True)\n",
    "                                    cv=skfold, \n",
    "                                    n_jobs=-1, # only 1 to avoid overload in parallel jobs, in non-par jobs it could be -1\n",
    "                                    return_estimator=True, # if you need the model to estimate on another test set\n",
    "                                    return_train_score=False,\n",
    "                                    )\n",
    "                            \n",
    "                pairwise_decoding_accuracies[(cond1,cond2)] = np.mean(cvs['test_score'])\n",
    "                pairwise_decoding_accuracies_std[(cond1,cond2)] = np.std(cvs['test_score'])\n",
    "                pairwise_decoding_estimators[(cond1,cond2)] = cvs['estimator']\n",
    "\n",
    "\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~ Save the decoding accuracy\n",
    "        save_folder = f\"{COMPUTE_DIR}/data/{group}/processed/{modality}/{subject}\"\n",
    "\n",
    "        filename = f\"EEGNet_accuracy_pairwise_{target_condition}_timewindow350.pkl\"\n",
    "        save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(pairwise_decoding_accuracies, f)\n",
    "        print(f\"{subject}: saved in {filename}\")\n",
    "        # ~~~~~~~~~~~~~~~~ Save the decoding accuracy ~~~~~~~~~~~~~~~~\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation\n",
    "        filename = f\"EEGNet_std_pairwise_{target_condition}_timewindow350.pkl\"\n",
    "        save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(pairwise_decoding_accuracies_std, f)\n",
    "        print(f\"{subject}: saved in {filename}\")\n",
    "        # ~~~~~~~~~~~~~~~~ Save the decoding accuracy standard deviation ~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~ Save the EEGNet estimation\n",
    "        filename = f\"EEGNet_estimator_pairwise_{target_condition}_timewindow350.pkl\"\n",
    "        save_path = os.path.join(save_folder, filename) #  a pickle file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(pairwise_decoding_estimators, f)\n",
    "        print(f\"{subject}: saved in {filename}\")\n",
    "        # ~~~~~~~~~~~~~~~~ Save the EEGNet estimation ~~~~~~~~~~~~~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
